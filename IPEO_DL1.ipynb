{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJUXQOC54AmY",
        "outputId": "bef74023-b5b3-49df-960e-01e0cc654ead"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.0+cu116)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.8/dist-packages (0.14.0)\n",
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.8/dist-packages (1.8.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.8/dist-packages (2.9.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.8/dist-packages (3.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (6.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (21.3)\n",
            "Requirement already satisfied: lightning-utilities!=0.4.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (0.4.2)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (2022.11.0)\n",
            "Requirement already satisfied: tensorboardX>=2.2 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (2.5.1)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (0.11.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (22.1.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.9)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX>=2.2->pytorch_lightning) (3.19.6)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (2.15.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.3.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.51.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (0.38.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (3.4.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.15.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!pip install torch torchvision torchtext pytorch_lightning tensorboard matplotlib tqdm wget"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import os\n",
        "if not os.path.exists(\"Alpine Land Cover.zip\"):\n",
        "    print(f\"downloading Alpine Land Cover.zip\")\n",
        "    urllib.request.urlretrieve(\"https://enacshare.epfl.ch/fQHAmeKDY6vnMxiEFRzyaP7csVXfNtg\", \"Alpine_Land_Cover.zip\")\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"Alpine_Land_Cover.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "LmFgbyLy4Qc7",
        "outputId": "820cc19d-6ad5-4ed7-b8d3-0a7421eaa023"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading Alpine Land Cover.zip\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b4f3c2c70f27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Alpine Land Cover.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"downloading Alpine Land Cover.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://enacshare.epfl.ch/fQHAmeKDY6vnMxiEFRzyaP7csVXfNtg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Alpine_Land_Cover.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgName = os.path.join('ipeo_data/alpine_label/', '25566_11089_label.tif') \n",
        "imgName2 = os.path.join('ipeo_data/rgb/', '25566_11089_rgb.tif') "
      ],
      "metadata": {
        "id": "Rb-ETgAN5-7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Xv__h7uk_IHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pimg = np.array(imgLabel)\n",
        "image3 = Image.fromarray(pimg*36)\n",
        "plt.imshow(image3)"
      ],
      "metadata": {
        "id": "71SiJh_XYkas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgLabel = Image.open(imgName)\n",
        "img = Image.open(imgName2)\n",
        "img=img.resize((200,200))\n",
        "img = img.convert(\"RGBA\")\n",
        "#lt.imshow(img)\n",
        "\n",
        "pimg = np.array(imgLabel)\n",
        "#print(pimg*36)\n",
        "image3 = Image.fromarray(pimg*36)\n",
        "rgb_im = image3.convert(\"RGBA\")\n",
        "blended_image = Image.blend(img, rgb_im, 1)\n",
        "plt.imshow(blended_image)"
      ],
      "metadata": {
        "id": "xotB8cUg_NDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "import csv\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "import torch\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "default_transform =  T.Compose([\n",
        "        T.Resize((200, 200)),\n",
        "        T.ToTensor()])\n",
        "\n",
        "class LandCoverData(Dataset):\n",
        "\n",
        "    # mapping between label class names and indices\n",
        "    LABEL_CLASSES = {\n",
        "      'Grass and other': 0,\n",
        "      'Wald': 1,\n",
        "      'Bushes and sparse forest': 2,\n",
        "      'Water and wetlands': 3,\n",
        "      'Glaciers and permanent snow': 4,\n",
        "      'Sparse rocks (rocks mixed with grass)': 5,\n",
        "      'Loose rocks, scree': 6,\n",
        "      'Bed rocks': 7\n",
        "    }\n",
        "\n",
        "    label_transform = T.ToTensor()\n",
        "\n",
        "    def __init__(self, transforms=None, split='train', ):\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # prepare data\n",
        "        self.data = []     \n",
        "\n",
        "        with open(f'ipeo_data/splits/{split}.csv', newline='') as csvfile:\n",
        "          csvReader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
        "          for row in csvReader:\n",
        "              #print(row)\n",
        "              rowStr=row[0]\n",
        "              if (rowStr==\"25595_11025\"):\n",
        "                continue\n",
        "              imgNameRGB = os.path.join('ipeo_data/rgb/', f'{rowStr}_rgb.tif')\n",
        "              imgNameLabel = os.path.join('ipeo_data/alpine_label/', f'{rowStr}_label.tif')\n",
        "              self.data.append((\n",
        "                  imgNameRGB,\n",
        "                  imgNameLabel\n",
        "              ))\n",
        "\n",
        "    #TODO: please provide the remaining functions required for the torch.utils.data.Dataset class.\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def __getitem__(self, x):\n",
        "        imgName, imgLabel = self.data[x]\n",
        "\n",
        "        img = Image.open(imgName)\n",
        "        label = Image.open(imgLabel)\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "        return img, self.label_transform(label)"
      ],
      "metadata": {
        "id": "wgdjhiEJ4tvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = LandCoverData(transforms=default_transform, split=\"train\")\n",
        "val_dataset = LandCoverData(transforms=default_transform, split=\"val\")"
      ],
      "metadata": {
        "id": "1FnZ3uBkKjUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# TODO create a training data dataloader with the specifications above\n",
        "train_dl = DataLoader(train_dataset, BATCH_SIZE, True, num_workers=2)\n",
        "# num_workers 8 default but 2 on colab\n",
        "val_dl = DataLoader(val_dataset, BATCH_SIZE, False, num_workers=2)"
      ],
      "metadata": {
        "id": "YwTiUf4sLKhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "26hmkEfIMWcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary packages\n",
        "from torch.nn import ConvTranspose2d\n",
        "from torch.nn import Conv2d\n",
        "from torch.nn import MaxPool2d\n",
        "from torch.nn import Module\n",
        "from torch.nn import ModuleList\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import BatchNorm2d\n",
        "from torchvision.transforms import CenterCrop\n",
        "from torch.nn import functional as F\n",
        "import torch"
      ],
      "metadata": {
        "id": "1EzodPiWQLPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(Module):\n",
        "\tdef __init__(self, inChannels, outChannels):\n",
        "\t\tsuper().__init__()\n",
        "\t\t# store the convolution and RELU layers\n",
        "\t\tself.conv1 = Conv2d(inChannels, outChannels, 3)\n",
        "\t\tself.batchNorm = BatchNorm2d(outChannels)\n",
        "\t\tself.relu = ReLU()\n",
        "\tdef forward(self, x):\n",
        "\t\t# apply CONV => RELU => CONV block to the inputs and return it\n",
        "\t\treturn self.relu(self.batchNorm(self.conv1(x)))"
      ],
      "metadata": {
        "id": "OvF1-KtRQLht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(Module):\n",
        "\tdef __init__(self, channels=(3,64,128,256,512,1024)):\n",
        "\t\tsuper().__init__()\n",
        "\t\t# store the encoder blocks and maxpooling layer\n",
        "\t\tself.encoderStep = ModuleList(\n",
        "\t\t\t[Block(channels[i], channels[i + 1])\n",
        "\t\t\t \tfor i in range(len(channels) - 1)])\n",
        "\t\tself.pool = MaxPool2d(kernel_size=3, stride=2)\n",
        "\t\n",
        "\tdef forward(self, x):\n",
        "\t\t# initialize an empty list to store the intermediate outputs\n",
        "\t\tblockOutputs = []\n",
        "\t\t# loop through the encoder blocks\n",
        "\t\tfor step in self.encoderStep:\n",
        "\t\t\t# pass the inputs through the current encoder block, store\n",
        "\t\t\t# the outputs, and then apply maxpooling on the output\n",
        "\t\t\tx = step(x)\n",
        "\t\t\tblockOutputs.append(x)\n",
        "\t\t\tx = self.pool(x)\n",
        "\t\t# return the list containing the intermediate outputs\n",
        "\t\treturn blockOutputs"
      ],
      "metadata": {
        "id": "CpvR_dW-QTN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(Module):\n",
        "\tdef __init__(self, channels=(1024, 512, 256, 128, 64)):\n",
        "\t\tsuper().__init__()\n",
        "\t\t# initialize the number of channels, upsampler blocks, and\n",
        "\t\t# decoder blocks\n",
        "\t\tself.channels = channels\n",
        "\t\tself.fusingConvs = ModuleList(\n",
        "\t\t\t[ConvTranspose2d(channels[i], channels[i + 1], 2, 2)\n",
        "\t\t\t \tfor i in range(len(channels) - 1)])\n",
        "\t\tself.decoderStep = ModuleList(\n",
        "\t\t\t[Block(channels[i], channels[i + 1])\n",
        "\t\t\t \tfor i in range(len(channels) - 1)])\n",
        "\t\n",
        "\tdef forward(self, x, encFeatures):\n",
        "\t\t# loop through the number of channels\n",
        "\t\tfor i in range(len(self.channels) - 1):\n",
        "\t\t\t# pass the inputs through the upsampler blocks\n",
        "\t\t\tx = self.fusingConvs[i](x)\n",
        "\t\t\t# crop the current features from the encoder blocks,\n",
        "\t\t\t# concatenate them with the current upsampled features,\n",
        "\t\t\t# and pass the concatenated output through the current\n",
        "\t\t\t# decoder block\n",
        "\n",
        "\t\t\t# STACKING\n",
        "\t\t\tencFeat = self.crop(encFeatures[i], x)\n",
        "\t\t\tx = torch.cat([x, encFeat], dim=1)\n",
        "\n",
        "\t\t\tx = self.decoderStep[i](x)\n",
        "\t\t# return the final decoder output\n",
        "\t\treturn x\n",
        "\tdef crop(self, encFeatures, x):\n",
        "\t\t# grab the dimensions of the inputs, and crop the encoder\n",
        "\t\t# features to match the dimensions\n",
        "\t\t(_, _, H, W) = x.shape\n",
        "\t\tencFeatures = CenterCrop([H, W])(encFeatures)\n",
        "\t\t# return the cropped features\n",
        "\t\treturn encFeatures"
      ],
      "metadata": {
        "id": "VKDs-xHuQaRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_IMAGE_HEIGHT=200\n",
        "INPUT_IMAGE_WIDTH=200"
      ],
      "metadata": {
        "id": "oIIA9ctCQvFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(Module):\n",
        "  def __init__(self, encChannels=(3,64,128,256,512,1024),\n",
        "\t\t decChannels=(1024, 512, 256, 128, 64),\n",
        "\t\t nbClasses=1, retainDim=True,\n",
        "\t\t outSize=(INPUT_IMAGE_HEIGHT,  INPUT_IMAGE_WIDTH)):\n",
        "      super().__init__()\n",
        "      # initialize the encoder and decoder\n",
        "      self.encoder = Encoder(encChannels)\n",
        "      self.decoder = Decoder(decChannels)\n",
        "      # initialize the regression head and store the class variables\n",
        "      self.head = Conv2d(decChannels[-1], nbClasses, 1)\n",
        "      self.retainDim = retainDim\n",
        "      self.outSize = outSize\n",
        "\n",
        "  def forward(self, x):\n",
        "      # grab the features from the encoder\n",
        "      encFeatures = self.encoder(x)\n",
        "      # pass the encoder features through decoder making sure that\n",
        "      # their dimensions are suited for concatenation\n",
        "      decFeatures = self.decoder(encFeatures[::-1][0],\n",
        "        encFeatures[::-1][1:])\n",
        "      # pass the decoder features through the regression head to\n",
        "      # obtain the segmentation mask\n",
        "      map = self.head(decFeatures)\n",
        "      # check to see if we are retaining the original output\n",
        "      # dimensions and if so, then resize the output to match them\n",
        "      if self.retainDim:\n",
        "        map = F.interpolate(map, self.outSize)\n",
        "      # return the segmentation map\n",
        "      return map"
      ],
      "metadata": {
        "id": "D5HKzsk3QdMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.nn import BCELoss\n",
        "from torch.nn import MSELoss\n",
        "\n",
        "from torch.optim import Adam\n",
        "from torch.optim import SGD\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imutils import paths\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import time"
      ],
      "metadata": {
        "id": "1CUM_WOzSbDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\"\n",
        "INIT_LR = 0.001\n",
        "NUM_EPOCHS = 40"
      ],
      "metadata": {
        "id": "_3aaRBoERw18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize our UNet model\n",
        "unet = UNet(nbClasses=8).to(DEVICE)\n",
        "# initialize loss function and optimizer\n",
        "lossFunc = BCELoss()\n",
        "opt = SGD(unet.parameters(), lr=INIT_LR)\n",
        "# calculate steps per epoch for training and test set\n",
        "trainSteps = len(train_dataset) // BATCH_SIZE\n",
        "testSteps = len(val_dataset) // BATCH_SIZE\n",
        "# initialize a dictionary to store training history\n",
        "H = {\"train_loss\": [], \"test_loss\": []}"
      ],
      "metadata": {
        "id": "UR2yiJSiQnmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ECLATE 25595_11025_label.tif"
      ],
      "metadata": {
        "id": "TQAwdDpASFAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loop over epochs\n",
        "print(\"[INFO] training the network...\")\n",
        "startTime = time.time()\n",
        "for e in tqdm(range(NUM_EPOCHS)):\n",
        "    # set the model in training mode\n",
        "    unet.train()\n",
        "    # initialize the total training and validation loss\n",
        "    totalTrainLoss = 0\n",
        "    totalTestLoss = 0\n",
        "    # loop over the training set\n",
        "    for (i, (x, y)) in enumerate(train_dl):\n",
        "        # send the input to the device\n",
        "        (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
        "        # perform a forward pass and calculate the training loss\n",
        "        pred = unet(x)\n",
        "        probs = torch.softmax(pred, dim=1)\n",
        "        preds = torch.argmax(probs, dim=1).unsqueeze(1)\n",
        "\n",
        "        \n",
        "        #y=y.to(torch.float32)\n",
        "        #preds=preds.to(torch.float32)\n",
        "        #print(pred)\n",
        "        #print(y)\n",
        "        \"\"\"\n",
        "        print(pred.shape)\n",
        "        print(x.shape)\n",
        "        print(y.shape)\n",
        "        print(pred.unsqueeze(1).shape)\n",
        "        print(pred)\n",
        "        print(y)\n",
        "        raise Exception(\"oui\")\n",
        "        \"\"\"\n",
        "        loss = lossFunc(pred, y)\n",
        "        \n",
        "        # first, zero out any previously accumulated gradients, then\n",
        "        # perform backpropagation, and then update model parameters\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        # add the loss to the total training loss so far\n",
        "        totalTrainLoss += loss\n",
        "    # switch off autograd\n",
        "    with torch.no_grad():\n",
        "        # set the model in evaluation mode\n",
        "        unet.eval()\n",
        "        # loop over the validation set\n",
        "        for (x, y) in val_dl:\n",
        "          # send the input to the device\n",
        "          (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
        "          # make the predictions and calculate the validation loss\n",
        "          pred = unet(x)\n",
        "          #pred=pred.to(torch.float32)\n",
        "          y=y.to(torch.float32)\n",
        "          totalTestLoss += lossFunc(pred, y)\n",
        "    # calculate the average training and validation loss\n",
        "    avgTrainLoss = totalTrainLoss / trainSteps\n",
        "    avgTestLoss = totalTestLoss / testSteps\n",
        "    # update our training history\n",
        "    H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
        "    H[\"test_loss\"].append(avgTestLoss.cpu().detach().numpy())\n",
        "    # print the model training and validation information\n",
        "    print(\"[INFO] EPOCH: {}/{}\".format(e + 1, NUM_EPOCHS))\n",
        "    print(\"Train loss: {:.6f}, Test loss: {:.4f}\".format(\n",
        "      avgTrainLoss, avgTestLoss))\n",
        "# display the total time needed to perform the training\n",
        "endTime = time.time()\n",
        "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
        "\tendTime - startTime))"
      ],
      "metadata": {
        "id": "zqvwoeJ4SvJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
        "plt.plot(H[\"test_loss\"], label=\"test_loss\")\n",
        "plt.title(\"Training Loss on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"lower left\")"
      ],
      "metadata": {
        "id": "Lg_QZnjKT0EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "so50KIpVVvjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "__PQf393Vv-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import cv2\n",
        "import os\n",
        "def prepare_plot(origImage, origMask, predMask):\n",
        "\t# initialize our figure\n",
        "\tfigure, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 10))\n",
        "\t# plot the original image, its mask, and the predicted mask\n",
        "\tax[0].imshow(origImage)\n",
        "\tax[1].imshow(origMask)\n",
        "\tax[2].imshow(predMask)\n",
        "\t# set the titles of the subplots\n",
        "\tax[0].set_title(\"Image\")\n",
        "\tax[1].set_title(\"Original Mask\")\n",
        "\tax[2].set_title(\"Predicted Mask\")\n",
        "\t# set the layout of the figure and display it\n",
        "\tfigure.tight_layout()\n",
        "\tfigure.show()"
      ],
      "metadata": {
        "id": "43ou777hVwJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MASK_DATASET_PATH = \"ipeo_data/alpine_label\"\n",
        "THRESHOLD = 10"
      ],
      "metadata": {
        "id": "TlDQ5xAkV64K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(model, imVal, trueLabel):\n",
        "    # set model to evaluation mode\n",
        "    model.eval()\n",
        "    # turn off gradient tracking\n",
        "    with torch.no_grad():\n",
        "      # load the image from disk, swap its color channels, cast it\n",
        "      # to float data type, and scale its pixel values\n",
        "      image = cv2.imread(imVal)\n",
        "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "      image = image.astype(\"float32\") / 255.0\n",
        "      # resize the image and make a copy of it for visualization\n",
        "      image = cv2.resize(image, (200, 200))\n",
        "      orig = image.copy()\n",
        "      # find the filename and generate the path to ground truth\n",
        "      # mask\n",
        "      groundTruthPath = trueLabel\n",
        "      # load the ground-truth segmentation mask in grayscale mode\n",
        "      # and resize it\n",
        "      pimg = Image.open(groundTruthPath)\n",
        "      pimg = np.array(pimg)\n",
        "      gtMask = Image.fromarray(pimg*36)\n",
        "          # make the channel axis to be the leading one, add a batch\n",
        "      # dimension, create a PyTorch tensor, and flash it to the\n",
        "      # current device\n",
        "      image = np.transpose(image, (2, 0, 1))\n",
        "      image = np.expand_dims(image, 0)\n",
        "      image = torch.from_numpy(image).to(DEVICE)\n",
        "      # make the prediction, pass the results through the sigmoid\n",
        "      # function, and convert the result to a NumPy array\n",
        "      predMask = model(image).squeeze()\n",
        "      #predMask = torch.sigmoid(predMask)\n",
        "      predMask = predMask.cpu().numpy()\n",
        "      \n",
        "      # filter out the weak predictions and convert them to integers\n",
        "      #print((predMask>THRESHOLD)*200)\n",
        "      #predMask = (predMask > THRESHOLD) * 255\n",
        "      #print(predMask[0:10])\n",
        "      predMask = Image.fromarray(predMask*36)\n",
        "      #predMask = predMask.astype(np.uint8)\n",
        "      #print(predMask)\n",
        "      # prepare a plot for visualization\n",
        "      prepare_plot(orig, gtMask, predMask)"
      ],
      "metadata": {
        "id": "41zDBPT1VzXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v-w9VTljYALN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the image paths in our testing file and randomly select 10\n",
        "# image paths\n",
        "print(\"[INFO] loading up test image paths...\")\n",
        "test_dataset = LandCoverData(transforms=default_transform, split=\"test\")\n",
        "\n",
        "# iterate over the randomly selected test image paths\n",
        "for imVal, trueLabel in test_dataset.data[100:103]:\n",
        "\t# make predictions and visualize the results\n",
        "  make_predictions(unet, imVal, trueLabel)"
      ],
      "metadata": {
        "id": "OWe-EI3ZV49H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "90S4YaEBXF0w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}