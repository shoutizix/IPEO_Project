{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740be313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USEFUL WHEN RUNNING ON CLUSTER\n",
    "import sys\n",
    "!pip install torch torchvision torchsummary torchtext pytorch_lightning tensorboard matplotlib tqdm datetime time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1516afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eddd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn import MSELoss\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim import SGD\n",
    "from torch.optim import RMSprop\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from src.model import UNet\n",
    "from src.dataloader import LandCoverData, transformsNorm, transformsNormAugmentedColoJitter\n",
    "import src.loss as lossPY\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d48a82f",
   "metadata": {},
   "source": [
    "# 1 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a4fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path=\"../\"\n",
    "path=\"/scratch/izar/damiani/\"\n",
    "#path=\"/scratch/izar/nkaltenr/\"\n",
    "\n",
    "# use_augmented should be True,\n",
    "# if you set use_restricted to True.\n",
    "# Otherwise wrong normalization.\n",
    "\n",
    "transformsTrain=transformsNorm(use_augmented=True,\n",
    "                               use_restricted=False,\n",
    "                               flag_plot=False)\n",
    "\n",
    "train_dataset = LandCoverData(path,\n",
    "                              transforms=transformsTrain,\n",
    "                              split=\"train\",\n",
    "                              ignore_last_number=11,\n",
    "                              use_augmented=True,\n",
    "                              restrict_classes=False)\n",
    "\n",
    "val_dataset = LandCoverData(path, \n",
    "                            transforms=transformsTrain,\n",
    "                            split=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08078119",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# num_workers 8 default but 2 on colab\n",
    "train_dl = DataLoader(train_dataset, BATCH_SIZE, True, drop_last=True)\n",
    "val_dl = DataLoader(val_dataset, BATCH_SIZE, False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7dc93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_IMAGE_HEIGHT=200\n",
    "INPUT_IMAGE_WIDTH=200\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "#DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a19141",
   "metadata": {},
   "outputs": [],
   "source": [
    "focal_loss = torch.hub.load(\n",
    "\t'adeelh/pytorch-multi-class-focal-loss',\n",
    "\tmodel='focal_loss',\n",
    "\talpha=None,\n",
    "\tgamma=2,\n",
    "\treduction='mean',\n",
    "\tdevice=DEVICE,\n",
    "\tdtype=torch.float32,\n",
    "\tforce_reload=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891cc4e1",
   "metadata": {},
   "source": [
    "# 2 Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef91167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Entropy Loss\n",
    "ce = CrossEntropyLoss()\n",
    "\n",
    "# Weighted Cross Entropy Loss\n",
    "wieght_freq = [0.38135882, 0.97431312, 1.02707798, 3.17324418, 1.74480126, 1.11790711, 0.51357981, 0.52475398]\n",
    "class_weights_ce = torch.FloatTensor(wieght_freq).to(DEVICE)\n",
    "cew = CrossEntropyLoss(weight=class_weights_ce)\n",
    "\n",
    "# Focal Loss\n",
    "focal_loss = torch.hub.load(\n",
    "\t'adeelh/pytorch-multi-class-focal-loss',\n",
    "\tmodel='focal_loss',\n",
    "\talpha=None,\n",
    "\tgamma=2,\n",
    "\treduction='mean',\n",
    "\tdevice=DEVICE,\n",
    "\tdtype=torch.float32,\n",
    "\tforce_reload=False\n",
    ")\n",
    "\n",
    "# Intersection Over Union Loss\n",
    "iou=lossPY.mIoULoss(n_classes=8).to(DEVICE)\n",
    "\n",
    "\n",
    "def UnetLoss(preds, targets):\n",
    "    #print(f\"preds : {preds.shape}\")\n",
    "    #print(f\"targets : {targets.shape}\")\n",
    "    ce_loss = ce(preds, targets)\n",
    "    #cew_loss = cew(preds, targets)\n",
    "    #iou_loss = iou(preds, targets)\n",
    "    #loss = focal_loss(preds, targets)\n",
    "\n",
    "    acc = (torch.max(preds, 1)[1] == targets).float().mean()\n",
    "    return ce_loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9697f33",
   "metadata": {},
   "source": [
    "# 3 Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f043e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our UNet model\n",
    "unet = UNet(nbClasses=8).to(DEVICE)\n",
    "\n",
    "# Use Pretrained DeepLabV3 model:\n",
    "# You also need to modify train loop (follows the instructions in the cell)\n",
    "\"\"\"\n",
    "unet = deeplabv3_resnet101(pretrained=True, progress=True)\n",
    "\n",
    "flag_train_only_last_layer=False\n",
    "\n",
    "if flag_train_only_last_layer:\n",
    "    for param in unet.parameters():\n",
    "        param.requires_grad=False\n",
    "unet.classifier = DeepLabHead(2048, 8)\n",
    "unet=unet.to(DEVICE)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d7c727",
   "metadata": {},
   "source": [
    "# 4 HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973cfaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize loss function\n",
    "lossFunc = UnetLoss\n",
    "\n",
    "INIT_LR = 1e-3\n",
    "INIT_MOMENTUM = 0.9\n",
    "\n",
    "# Choose Optimizer\n",
    "opt = SGD(unet.parameters(), lr=INIT_LR, momentum=INIT_MOMENTUM)\n",
    "#opt = Adam(unet.parameters(), lr=INIT_LR, weight_decay=1e-6)\n",
    "#opt = RMSprop(unet.parameters(), lr=INIT_LR, momentum=INIT_MOMENTUM, weight_decay=1e-6)\n",
    "\n",
    "# Scheduler\n",
    "flag_scheduler=True\n",
    "scheduler = ReduceLROnPlateau(opt, 'max', patience=5)\n",
    "\n",
    "# Calculate steps per epoch for training and validation set\n",
    "trainSteps = len(train_dataset) // BATCH_SIZE\n",
    "valSteps = len(val_dataset) // BATCH_SIZE\n",
    "\n",
    "# Dictionary to store training history\n",
    "H = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "bestValLoss = float('inf')\n",
    "bestValAcc = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca4c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prev H to run in 10 hours batch\n",
    "#H = torch.load('enter_filename_H.pth', map_location=torch.device(DEVICE))\n",
    "#unet.load_state_dict(torch.load('enter_filename_model.pth', map_location=torch.device(DEVICE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fb9796",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_for_save = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee6a3cd",
   "metadata": {},
   "source": [
    "# 5 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db3fc99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loop over epochs\n",
    "NUM_EPOCHS = 100\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "for e in tqdm(range(NUM_EPOCHS)):\n",
    "    # set the model in training mode\n",
    "    unet.train()\n",
    "    # initialize the total training and validation loss\n",
    "    totalTrainLoss = 0\n",
    "    totalValLoss = 0\n",
    "    totalTrainAcc = 0\n",
    "    totalValAcc = 0\n",
    "    # loop over the training set\n",
    "    for (i, (x, y)) in enumerate(train_dl):\n",
    "        # send the input to the device\n",
    "        (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
    "        # perform a forward pass and calculate the training loss\n",
    "        pred = unet(x)\n",
    "        y = y.to(torch.long)\n",
    "        y = y.squeeze()\n",
    "        \n",
    "        # If you are using Pretrained DeepLabV3 model:\n",
    "        #loss, acc = lossFunc(pred['out'], y)\n",
    "        # Otherwise\n",
    "        loss, acc = lossFunc(pred, y)\n",
    "        \n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        # add the loss to the total training loss so far\n",
    "        totalTrainLoss += loss\n",
    "        totalTrainAcc += acc\n",
    "    # switch off autograd\n",
    "    with torch.no_grad():\n",
    "        unet.eval()\n",
    "        # loop over the validation set\n",
    "        for (x, y) in val_dl:\n",
    "            # send the input to the device\n",
    "            (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
    "            # make the predictions and calculate the validation loss\n",
    "            pred = unet(x)\n",
    "            y = y.to(torch.long)\n",
    "            y = y.squeeze()\n",
    "            \n",
    "            # If you are using Pretrained DeepLabV3 model:\n",
    "            #loss, acc = lossFunc(pred['out'], y)\n",
    "            # Otherwise\n",
    "            loss, acc = lossFunc(pred, y)\n",
    "            totalValLoss += loss\n",
    "            totalValAcc += acc\n",
    "            \n",
    "    # calculate the average training and validation loss\n",
    "    avgTrainLoss = totalTrainLoss / trainSteps\n",
    "    avgValLoss = totalValLoss / valSteps\n",
    "    avgTrainAcc = totalTrainAcc / trainSteps\n",
    "    avgValAcc = totalValAcc / valSteps\n",
    "    \n",
    "    if flag_scheduler:\n",
    "        scheduler.step(avgValAcc)\n",
    "    \n",
    "    print(f\" learning_rate={opt.param_groups[0]['lr']}\")\n",
    "    \n",
    "    # update our training history\n",
    "    H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "    H[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "    H[\"train_acc\"].append(avgTrainAcc.cpu().detach().numpy())\n",
    "    H[\"val_acc\"].append(avgValAcc.cpu().detach().numpy())\n",
    "    # print the model training and validation information\n",
    "    print(\"[INFO] EPOCH: {}/{}\".format(e + 1, NUM_EPOCHS))\n",
    "    print(\"       train loss: {:.4f}, val loss: {:.4f}\".format(\n",
    "      avgTrainLoss, avgValLoss))\n",
    "    print(\"       train acc: {:.4f}%, val acc: {:.4f}%\".format(\n",
    "      avgTrainAcc, avgValAcc))\n",
    "    # Save the best model (the one that has the lowest loss for validation)\n",
    "    if (bestValLoss == -1) or (bestValLoss > avgValLoss):\n",
    "        bestValLoss = avgValLoss\n",
    "        print(\"best loss => saving\")\n",
    "        torch.save(unet.state_dict(), f'best_model_{name_for_save}_loss.pth')\n",
    "    if (bestValAcc < avgValAcc):\n",
    "        bestValAcc = avgValAcc\n",
    "        print(\"best acc => saving\")\n",
    "        torch.save(unet.state_dict(), f'best_model_acc_{name_for_save}_loss.pth')\n",
    "    if ((e+1)%50 == 0):\n",
    "        epoch_name = e+1\n",
    "        print(\"SAVING\")\n",
    "        torch.save(unet.state_dict(), f\"unet_model_epoch_{epoch_name}_{name_for_save}_loss.pth\")\n",
    "        torch.save(H, f\"unet_model_epoch_{epoch_name}_{name_for_save}_H.pth\")\n",
    "        \n",
    "# display the total time needed to perform the training\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628fd0a9",
   "metadata": {},
   "source": [
    "# 6 Save and Results on train/val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9867c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.datetime.now()\n",
    "date_ymd = date.date()\n",
    "date_hm = f\"{date.hour}:{date.minute}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e72473",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(H[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Training Loss on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(f\"train_val_loss_{date_ymd}_{date_hm}_{name_for_save}_loss.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60a71db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(H[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(f\"train_val_acc_{date_ymd}_{date_hm}_{name_for_save}_loss.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef9d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(unet.state_dict(), f\"unet_model_{date_ymd}_{date_hm}_{name_for_save}_loss.pth\")\n",
    "torch.save(unet, f\"unet_model_{date_ymd}_{date_hm}_{name_for_save}_loss.pt\")\n",
    "torch.save(H, f\"unet_model_{date_ymd}_{date_hm}_{name_for_save}_loss_dict.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddf9bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f519a38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d96973f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62524d29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
